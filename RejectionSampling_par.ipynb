{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/My\\ Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVSampler():\n",
    "    def __init__(self, reference, inference, target_cols):\n",
    "        \"\"\"\n",
    "        reference : pd.DataFrame. 모집단으로 사용될 set.(test를 넣으면 됨)\n",
    "        inference : pd.DataFrame. 배경집단으로 사용된 inference set\n",
    "        target_cols : array-like. 해당 column들에 대해서만 확률 계산\n",
    "        \"\"\"\n",
    "        self._pdf = {}\n",
    "        self._cnt = np.zeros(reference.shape[0])\n",
    "        self.reference = reference.copy()\n",
    "        del reference\n",
    "        self.inference = inference.copy()\n",
    "        del inference\n",
    "        self.target_cols = target_cols\n",
    "\n",
    "        bins = int(1+3.322*np.log10(self.inference.shape[0]))\n",
    "        for col in self.target_cols:\n",
    "            p, bin_edge = np.histogram(self.inference[col].values,\n",
    "                                       bins=np.min([bins, len(self.inference[col].unique())]),\n",
    "                                       density=True)\n",
    "            self._pdf[col] = (p, bin_edge)\n",
    "\n",
    "        for col in self.target_cols:\n",
    "            bin_edge = self._pdf[col][1]\n",
    "            b = (bin_edge[1:] + bin_edge[:-1])/2\n",
    "            lcond = self.reference[col].values>=b.min()\n",
    "            rcond = self.reference[col].values<=b.max()\n",
    "            self._cnt += (lcond*rcond*1)\n",
    "\n",
    "    def build(self, row_min = -1, outlier = 0):\n",
    "        \"\"\"\n",
    "        row_min : int. sampling에 사용될 reference의 최소 row 갯수\n",
    "        outlier : int. ref/inf 의 target_col 분포를 비교할 때, 고려하지 않을 column의 갯수\n",
    "        모든 target_col이 ref/inf 분포가 겹치지 않을 수 있기 때문에 일정 row 갯수가 확보될 때 까지 증가\n",
    "        \"\"\"\n",
    "        if row_min  < 0:\n",
    "            row_min = int(self.inference.shape[0] * 0.1)\n",
    "\n",
    "        nsize = len(np.where(self._cnt>=len(self.target_cols)-outlier)[0])\n",
    "        while (nsize <= row_min) and (outlier <= len(self.target_cols)):\n",
    "            outlier += 1\n",
    "            nsize = len(np.where(self._cnt>=len(self.target_cols)-outlier)[0])\n",
    "        self._outlier = outlier\n",
    "\n",
    "        self.log_likelihood = np.zeros(self.reference.shape[0])\n",
    "        for i in range(self.reference.shape[0]):\n",
    "            row_vector = self.reference.iloc[i, :]\n",
    "            if self._cnt[i]>=len(self.target_cols)-outlier:\n",
    "                p = np.array([self.interp_pdf(self._pdf[col], row_vector[col]) for col in self.target_cols])\n",
    "                self.log_likelihood[i] = np.sum(np.log10(p[np.where(p!=0)[0]]))\n",
    "                # np.power(10, np.sum(np.log10(p[np.where(p!=0)[0]])))\n",
    "\n",
    "    @staticmethod\n",
    "    def interp_pdf(pdf, x):\n",
    "        p, bin_edge = pdf\n",
    "        b = (bin_edge[1:] + bin_edge[:-1])/2\n",
    "        if x<=b.min() or x>=b.max():\n",
    "            return 0\n",
    "        f = interp1d(b, p, kind=\"linear\")\n",
    "        y = f(x)\n",
    "        return y\n",
    "\n",
    "    def sample(self, n=-1, seed=None, return_index=False):\n",
    "        \"\"\"\n",
    "        n : int. sample의 크기\n",
    "        return_index : bool. True일 경우 sample의 index를 array로 return, False일 경우 sample을 pd.DataFrame으로 return. 기본은 False\n",
    "        return : pd.DataFrame / array-like\n",
    "        \"\"\"\n",
    "        n = n if n>0 else self.inference.shape[0]//10\n",
    "        prob = np.power(10, self.log_likelihood)\n",
    "        prob /= np.sum(prob)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.choice(np.arange(0, self.reference.shape[0]), replace=True, p=prob, size=n)\n",
    "        if return_index:\n",
    "            return idx\n",
    "        return self.reference.iloc[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(slope, intercept, x, y):\n",
    "    return abs(slope*x-y+intercept)/np.sqrt(slope**2+1)\n",
    "def linear(x, a, b):\n",
    "    return a*x+b\n",
    "def chi2(slope, intercept, x, y):\n",
    "    dobs = dist(slope, intercept, x, y)**2\n",
    "    return np.sum(dobs)/ 2 / (len(x)-2)\n",
    "\n",
    "\n",
    "class PerformanceEstimator():\n",
    "    def __init__(self, drift, drift_smp, metric_smp):\n",
    "        self.drift = drift\n",
    "        self.drift_smp = drift_smp\n",
    "        self.metric_smp = metric_smp\n",
    "\n",
    "    def estimate(self, n = 5):\n",
    "        drift_list = self.drift_smp.keys()\n",
    "        metric_list = self.metric_smp.keys()\n",
    "        if len(self.drift[list(self.drift.keys())[0]])<n:\n",
    "            return self.metric_smp\n",
    "\n",
    "        metric_est = {}\n",
    "        for key, m in self.metric_smp.items():\n",
    "            metric_est[key] = self.correction(metric= m, drift =self.drift, drift_smp = self.drift_smp)\n",
    "        return metric_est\n",
    "\n",
    "    @staticmethod\n",
    "    def nlinearfit(x, y, th = 2):\n",
    "        x = np.array(x) if isinstance(x, list) else x\n",
    "        y = np.array(y) if isinstance(y, list) else y\n",
    "        m = (y.max() - y.min()) / (x.min() - x.max())\n",
    "        i = y.mean() - m * x.mean()\n",
    "        d, c2 = dist(m, i, x, y), chi2(m, i, x, y)\n",
    "        cnt = 0\n",
    "\n",
    "        while c2>th and m>0 and cnt<len(x)-4:\n",
    "            xn, yn = x[np.argsort(d)[:len(x)-cnt]], y[np.argsort(d)[:len(x)-cnt]]\n",
    "            popt, _ = curve_fit(linear, xn, yn)\n",
    "            m, i = popt[0], popt[1]\n",
    "            c2 = chi2(m, i, xn, yn)\n",
    "            cnt += 1\n",
    "        c2 = chi2(m, i, x, y)\n",
    "        return m, i, c2\n",
    "\n",
    "    @staticmethod\n",
    "    def correction(metric, drift, drift_smp):\n",
    "        metric_cor = 0\n",
    "        denominator = 0\n",
    "\n",
    "        for k in drift_smp.keys():\n",
    "            d, d_smp =  drift[k], drift_smp[k]\n",
    "            dbar = d[-1] - np.min(d)\n",
    "            dbar_smp = d_smp - np.min(d_smp)\n",
    "            r1, beta, chi2 = PerformanceEstimator.nlinearfit(dbar_smp, metric, 2)\n",
    "            metric_cor += (r1 * dbar + beta) / chi2\n",
    "            denominator +=  (1/chi2)\n",
    "\n",
    "        return metric_cor/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "def accuracy(args):\n",
    "    model, X, y = args[0], args[1], args[2]\n",
    "    return accuracy_score(model.predict(X), y)*100\n",
    "\n",
    "def f1(args):\n",
    "    model, X, y = args[0], args[1], args[2]\n",
    "    return f1_score(model.predict(X), y) *100\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def D3(args):\n",
    "    reference, inference, model = args[0], args[1], args[2]\n",
    "    y1 = np.zeros(reference.shape[0]).reshape(-1, 1)\n",
    "    y2 = np.ones(inference.shape[0]).reshape(-1, 1)\n",
    "    X = np.vstack((reference, inference))\n",
    "    y = np.vstack((y1, y2))\n",
    "    lr = LogisticRegression(solver=\"liblinear\", random_state=0, max_iter=5000).fit(X, y.ravel())\n",
    "    return roc_auc_score(y, lr.predict_proba(X)[:, 1])\n",
    "\n",
    "from scipy.stats import ks_2samp, entropy\n",
    "def uncertainty(args):\n",
    "    reference, inference, model = args[0], args[1], args[2]\n",
    "    entropy_ref = entropy(model.predict_proba(reference), axis = -1)\n",
    "    entropy_inf = entropy(model.predict_proba(inference), axis = -1)\n",
    "    d, p = ks_2samp(entropy_ref, entropy_inf)\n",
    "    return d\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "def FID(reference, inference, model):\n",
    "    act_ref, act_inf = [], []\n",
    "\n",
    "    for i in range(reference.shape[0]):\n",
    "        refr, infr = reference[i, :], inference[i, :]\n",
    "        vmin, vmax = min(refr.min(), infr.min()), max(refr.max(), infr.max())\n",
    "        bins = int(1+3.322*np.log10(max(len(refr), len(infr))))\n",
    "        refh, _ = np.histogram(refr, range=(vmin, vmax), bins=bins)\n",
    "        infh, _ = np.histogram(infr, range=(vmin, vmax), bins=bins)\n",
    "        act_ref.append(refh)\n",
    "        act_inf.append(infh)\n",
    "\n",
    "    act_ref = np.array(act_ref)\n",
    "    act_inf = np.array(act_inf)\n",
    "    mu1, sigma1 = np.mean(act_ref, axis=0), np.cov(act_ref, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(act_inf, axis=0), np.cov(act_inf, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    mat_dot = sigma1.dot(sigma2)\n",
    "    covmean = sqrtm(mat_dot)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run(df_train, df_test, df_live, target_cols, drop_cols, y, n_proc):\n",
    "\n",
    "    drift_checkers = [D3, uncertainty]\n",
    "    drift_list = [dc.__name__ for dc in drift_checkers]\n",
    "    metrics = [accuracy]\n",
    "    metric_list = [m.__name__ for m in metrics]\n",
    "\n",
    "\n",
    "    DB = {}\n",
    "    for metric in metrics:\n",
    "        DB[metric.__name__] = []\n",
    "        DB[metric.__name__+\"_smp\"] = []\n",
    "        DB[metric.__name__+\"_est\"] = []\n",
    "    for dc in drift_checkers:\n",
    "        DB[dc.__name__] = []\n",
    "        DB[dc.__name__+\"_smp\"] = []\n",
    "    DB[\"index\"] = []\n",
    "\n",
    "    X_train = df_train.drop(columns=drop_cols).values\n",
    "    y_train = df_train[y].values\n",
    "\n",
    "    model = RandomForestClassifier(max_depth=6).fit(X_train, y_train)\n",
    "\n",
    "    X_test = df_test.drop(columns=drop_cols).values\n",
    "    y_test = df_test[y].values\n",
    "\n",
    "\n",
    "\n",
    "    for dc in drift_checkers:\n",
    "        v = dc([X_train, X_test, model])\n",
    "        DB[dc.__name__].append(v)\n",
    "        DB[dc.__name__+\"_smp\"].append(v)\n",
    "    for m in metrics:\n",
    "        v = m([model, X_test, y_test])\n",
    "        DB[m.__name__].append(v)\n",
    "        DB[m.__name__+\"_smp\"].append(v)\n",
    "        DB[m.__name__+\"_est\"].append(v)\n",
    "    DB[\"index\"].append(-1)\n",
    "\n",
    "    print(\"DB setup\")\n",
    "    for i, live in enumerate(df_live):\n",
    "        if live.shape[0]>0:\n",
    "            DB[\"index\"].append(i)\n",
    "            Xlive = live.drop(columns=drop_cols).values\n",
    "            ylive = live[y].values\n",
    "\n",
    "            for dc in drift_checkers:\n",
    "                DB[dc.__name__].append(dc([X_train, Xlive, model]))\n",
    "\n",
    "            mvs = MVSampler(df_test, live, target_cols)\n",
    "            mvs.build(0)\n",
    "            sample = [mvs.sample(return_index=False) for _ in range(100)]\n",
    "\n",
    "            args_drift = [(X_train, sample[i].drop(columns=drop_cols).values, model) for i in range(100)]\n",
    "            for dc in drift_checkers:\n",
    "                if __name__==\"__main__\":\n",
    "                    with mp.Pool(8) as pool:\n",
    "                        res = pool.map(dc, args_drift)\n",
    "                DB[dc.__name__+\"_smp\"].append(np.mean(res))\n",
    "\n",
    "            args_metric = [(model, sample[i].drop(columns=drop_cols).values, sample[i][y].values) for i in range(100)]\n",
    "            for m in metrics:\n",
    "                if __name__==\"__main__\":\n",
    "                    with mp.Pool(n_proc) as pool:\n",
    "                        res = pool.map(m, args_metric)\n",
    "                DB[m.__name__+\"_smp\"].append(np.mean(res))\n",
    "\n",
    "\n",
    "            ### real metric\n",
    "            for m in metrics:\n",
    "                DB[m.__name__].append(m([model, Xlive, ylive]))\n",
    "\n",
    "            ### DB에서 load\n",
    "            DB_drift = {k:DB[k] for k in drift_list}\n",
    "            DB_drift_smp = {k:DB[k+\"_smp\"] for k in drift_list}\n",
    "            DB_metric_smp = {k:DB[k+\"_smp\"] for k in metric_list}\n",
    "\n",
    "            pe = PerformanceEstimator(\n",
    "            drift = DB_drift,\n",
    "            drift_smp = DB_drift_smp,\n",
    "            metric_smp = DB_metric_smp)\n",
    "\n",
    "            metric_est = pe.estimate(n = 5)\n",
    "            for k, v in metric_est.items():\n",
    "                if isinstance(v, list):\n",
    "                    DB[k+\"_est\"].append(v[-1])\n",
    "                else:\n",
    "                    DB[k+\"_est\"].append(v)\n",
    "    return DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 40000\n",
    "cor = [1.5, 1.2, 0.9 , 0.7, 0.5]\n",
    "y = (np.random.normal(0.5,0.5,100) > 0.5).astype(int)\n",
    "y = (np.random.normal(0.5,0.5,data_size) > 0.5).astype(int)\n",
    "x = np.zeros((data_size,5))\n",
    "for ix,cor_ in enumerate(cor):\n",
    "    x[:,ix] = y + np.random.normal(1,cor_,data_size)\n",
    "for i in range(400):\n",
    "    s = i*100\n",
    "    x[s:s+100,4] = y[s:s+100] + np.random.normal(1,0.5+(i/100),100)\n",
    "columns=['x0','x1','x2','x3','z0']\n",
    "df = pd.DataFrame(x,columns=columns)\n",
    "df[\"y\"] = y\n",
    "\n",
    "### Reference =split into=> train/test ###\n",
    "size = 5000\n",
    "df_train = df.iloc[:size, :]\n",
    "df_test = df.iloc[size:2*size, :]\n",
    "\n",
    "### Inference =split into=> indiv live set ###\n",
    "inference = df.iloc[2*size:, :]\n",
    "s = 1000\n",
    "live_set = [inference.iloc[i*s:(i+1)*s, :] for i in range(inference.shape[0]//s)]\n",
    "\n",
    "### Target/Drop/Y columns ###\n",
    "# target_cols : sampling 할 때 고려할 column list. feature importance 등을 사용하여 설정\n",
    "# drop_cols : X domain에서 제외할 column명\n",
    "# ylabel : y domain column명\n",
    "\n",
    "ylabel = \"y\"\n",
    "target_cols = df_train.drop(columns=ylabel).columns\n",
    "drop_cols = [\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_cols = np.genfromtxt(\"/content/drive/My Drive/kaggle/loan.txt\", delimiter='\\t', dtype=str)\n",
    "# df = pd.read_parquet(\"/content/drive/My Drive/kaggle/loan.parquet\")\n",
    "\n",
    "# from astropy.time import Time\n",
    "# df = df[(df.time>=Time(\"2013-01-01\").mjd) & (df.time<Time(\"2018-01-01\").mjd)]\n",
    "# dt = 30\n",
    "# df_train = df[df.time<Time(\"2014-01-01\").mjd]\n",
    "# df_test = df[(df.time>=Time(\"2014-01-01\").mjd) & (df.time<Time(\"2015-01-01\").mjd)]\n",
    "# live = [df[(df.time>=Time(\"2015-01-01\").mjd+i*dt) & (df.time<Time(\"2015-01-01\").mjd+(i+1)*dt)] for i in range(int((df.time.max()-Time(\"2015-01-01\").mjd)//dt))]\n",
    "# drop_cols = [\"time\", \"loan_paid\"]\n",
    "# y = \"loan_paid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB setup\n"
     ]
    }
   ],
   "source": [
    "DB = run(df_train, df_test, live_set, target_cols, drop_cols, ylabel, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "gs = GridSpec(nrows=2, ncols=1, hspace=0, wspace=0)\n",
    "name = \"accuracy\"\n",
    "axes = [plt.subplot(gs[i]) for i in range(2)]\n",
    "axes[0].plot(DB[\"index\"], DB[name], label=name+'_ori')\n",
    "axes[0].plot(DB[\"index\"], DB[name+\"_smp\"], label=name+\"_smp\")\n",
    "axes[0].plot(DB[\"index\"], DB[name+\"_est\"], label=name+\"_est\", linestyle=\"--\")\n",
    "\n",
    "axes[1].plot(DB[\"index\"], DB[\"uncertainty\"], label=\"unc\")\n",
    "axes[1].plot(DB[\"index\"], DB[\"uncertainty_smp\"], label=\"smp\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(min(DB[\"index\"]), max(DB[\"index\"]))\n",
    "    ax.grid()\n",
    "    ax.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4882541213f784677a7e5c9790aab938f4b3578eb781276b5a01fe382bc18811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
